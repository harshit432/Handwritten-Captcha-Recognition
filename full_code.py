# -*- coding: utf-8 -*-
"""Copy of Copy of mosaic_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1flpQhg485v370PZZWvRY8UK6urmcsBzj
"""

tensorflow_version 1.15

import tensorflow as tf
import os
import numpy as np
from mlxtend.data import loadlocal_mnist
import pandas as pd

#This will load the train and test data
x_train_data,y_train_data = loadlocal_mnist(
        images_path='/content/drive/My Drive/A_mosaic/EMNIST_b/emnist-balanced-train-images-idx3-ubyte', 
        labels_path='/content/drive/My Drive/A_mosaic/EMNIST_b/emnist-balanced-train-labels-idx1-ubyte')
x_test_data, y_test_data = loadlocal_mnist(
        images_path='/content/drive/My Drive/A_mosaic/EMNIST_b/emnist-balanced-test-images-idx3-ubyte', 
        labels_path='/content/drive/My Drive/A_mosaic/EMNIST_b/emnist-balanced-test-labels-idx1-ubyte')

# Convert data to numpy arrays and normalize images to the interval [0, 1]

x_train = np.array(x_train_data)/255.0
y_train = np.array(y_train_data)
x_test = np.array(x_test_data)/255.0
y_test = np.array(y_test_data)
print(x_train.shape)
print(x_test.shape)
x_train=x_train.reshape(x_train.shape[0],28,28)
x_test=x_test.reshape(x_test.shape[0],28,28)
import matplotlib.pyplot as plt
plt.imshow(x_train[0])

for t in range(112800):
  x_train[t]=np.transpose(x_train[t])
plt.imshow(x_train[0]) 
for t in range(18800):
  x_test[t]=np.transpose(x_test[t]) 
x_train=x_train.reshape(x_train.shape[0],28,28,1)
x_test=x_test.reshape(x_test.shape[0],28,28,1)

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import itertools

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import itertools

from keras.utils.np_utils import to_categorical # convert to one-hot-encoding
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,Activation
from keras.optimizers import RMSprop,Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.optimizers import SGD
from keras.callbacks import LearningRateScheduler
from keras.layers.normalization import BatchNormalization
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D
from keras.regularizers import l1

y_train=to_categorical(y_train,num_classes=47)
y_test=to_categorical(y_test,num_classes=47)

#from pyimagesearch.nn.conv import LeNet
model = Sequential()
inputShape = (28,28,1)
activation='relu'
model.add(Conv2D(20, 5, padding="same",input_shape=inputShape))
model.add(Activation(activation))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Conv2D(50, 5, padding="same"))
model.add(Activation(activation))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Flatten())
model.add(Dense(500))
model.add(Activation(activation))
		# define the second FC layer
model.add(Dense(47))
		# lastly, define the soft-max classifier
model.add(Activation("softmax"))

annealer = LearningRateScheduler(lambda x: 1e-7 * 0.9 ** x)
model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=["accuracy"])

gen = ImageDataGenerator(rotation_range=10,
                         width_shift_range=0.08, 
                         shear_range=0.3,
                         height_shift_range=0.08,
                         zoom_range=0.08,
                         featurewise_center=False, # set input mean to 0 over the dataset
                         samplewise_center=False,  # set each sample mean to 0
                         featurewise_std_normalization=False,  # divide inputs by std of the dataset
                         samplewise_std_normalization=False,  # divide each input by its std
                         zca_whitening=False,    # apply ZCA whitening
                         horizontal_flip=False,  # randomly flip images
                         vertical_flip=False)

gen.fit(x_train)
epochs=10
batch_size=50
learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', 
                                            patience=3, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)
history = model.fit_generator(gen.flow(x_train,y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (x_test,y_test),
                              verbose = 2, steps_per_epoch=x_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction])

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Predict the values from the validation dataset
y_pred = model.predict(x_test)
# Convert predictions classes to one hot vectors 
y_pred_classes = np.argmax(y_pred,axis = 1) 
# Convert validation observations to one hot vectors
y_true = np.argmax(y_test,axis = 1) 
# compute the confusion matrix
confusion_mtx = confusion_matrix(y_true, y_pred_classes) 
# plot the confusion matrix
plot_confusion_matrix(confusion_mtx, classes = range(47))

from tensorflow.python.keras.models import load_model
from tensorflow.python.keras.models import model_from_json

model_json = model.to_json()
with open("/content/drive/My Drive/A_mosaic/EMNIST_b/model.json", "w") as json_file:
    json_file.write(model_json)
#saves the model info as json file
    
model.save_weights("/content/drive/My Drive/A_mosaic/EMNIST_b/model.h5")
# Creates a HDF5 file 'model.h5'